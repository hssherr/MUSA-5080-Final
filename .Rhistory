col = colors[2], border = "grey30")
plot(x = weather_complete$relh,
y = weather_complete$wspd,
pch = 16,
xlab = "Relative Humidity (%)",
ylab = "Wind Speed (mph)",
main = "Wind Speed vs. Relative Humidity (2024)",
col = alpha("black", 0.1))
# import a non-exhaustive list of commercial, retail, and office businesses from OSM
offices <- opq(st_bbox(phl_boundary %>% st_transform(4326))) %>%
add_osm_feature(key = "office") %>%
osmdata_sf(.)
shops <- opq(st_bbox(phl_boundary %>% st_transform(4326))) %>%
add_osm_feature(key = "shop") %>%
osmdata_sf(.)
# extract points from osm object, transform to EPSG 2272, and filter to PHL boundary
offices_pts <- offices[["osm_points"]] %>%
st_transform(2272) %>%
st_filter(phl_boundary, .predicate = st_within) %>%
mutate(type = "office")
shops_pts <- shops[["osm_points"]] %>%
st_transform(2272) %>%
st_filter(phl_boundary, .predicate = st_within) %>%
mutate(type = "shop")
# plot business locations
ggplot() +
geom_sf(data = phl_census, color = "grey80", fill = "grey95") +
geom_sf(data = offices_pts, aes(color = "Offices"), alpha = 0.25, size = 0.5) +
geom_sf(data = shops_pts, aes(color = "Shops"), alpha = 0.25, size = 0.5) +
scale_color_manual(name = "Business Types", values = c("Offices" = colors[1], "Shops" = colors[2])) +
labs(title = "Businesses in Philadelphia by Type") +
theme_void()
# combine businesses into one dataset and isolate census tract geometries/GEOIDs
businesses <- rbind(offices_pts %>% select(osm_id, name, type),
shops_pts %>% select(osm_id, name, type))
phl_tracts <- phl_census %>% select(GEOID)
# calculate business counts and densities per census tract
businesses_dens <- phl_tracts %>%
mutate(business_cnt = lengths(st_intersects(., businesses)),
business_dens = as.numeric(business_cnt/st_area(.))*2.78784e+7)
# plot business densitites spatially
ggplot() +
geom_sf(data = businesses_dens,
aes(fill = business_dens),
color = NA) +
scale_fill_viridis(name = "Business Density (#/sqmi)") +
labs(title = "Census Tract Business Density in Philadelphia (2025)") +
theme_void()
# Count trips by station-hour
trips_panel <- indego %>%
group_by(interval60, start_station, start_lat, start_lon) %>%
summarize(Trip_Count = n()) %>%
ungroup()
# How many station-hour observations?
cat("Original count of rows in Trips Panel:", nrow(trips_panel))
# How many unique stations?
cat("Unique stations in the dataset:", length(unique(trips_panel$start_station)))
# How many unique hours?
cat("Unique hours in the dataset:", length(unique(trips_panel$interval60)))
# find the number of stations and hours we need to represent
n_stations <- length(unique(trips_panel$start_station))
n_hours <- length(seq(min(trips_panel$interval60), max(trips_panel$interval60), by = "hour"))
expected_rows <- n_stations * n_hours
cat("Expected panel rows:", format(expected_rows, big.mark = ","), "\n")
cat("Current rows:", format(nrow(trips_panel), big.mark = ","), "\n")
cat("Missing rows:", format(expected_rows - nrow(trips_panel), big.mark = ","), "\n")
# join trip counts to expanded grid
study_panel <-
expand.grid(interval60 = seq(min(trips_panel$interval60),
max(trips_panel$interval60),
by = "hour"),
start_station = unique(trips_panel$start_station)
) %>%
left_join(., trips_panel %>% select(-c(start_lat, start_lon)),
by = c("interval60", "start_station")) %>%
mutate(Trip_Count = replace_na(Trip_Count, 0))
# get station lat and lon columns from indego df
stn_coords <- indego %>%
select(start_station, start_lat, start_lon) %>%
group_by(start_station) %>%
slice_head(n=1) %>%
ungroup()
# get the census tract each station is in
stn_points_filt_panel <- st_join(stn_points_filt, phl_tracts, join = st_within) %>%
left_join(., stn_coords,
by = "start_station") %>%
select(-in_phl) %>%
st_drop_geometry()
# fill in station-level attributes and variables
study_panel <- study_panel %>%
left_join(., stn_points_filt_panel,
by = "start_station")
study_panel <- study_panel %>%
mutate(
week = week(interval60),
month = month(interval60, label = TRUE),
dotw = wday(interval60, label = TRUE),
dotw_simple = factor(dotw,
levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")),
hour = hour(interval60),
date = as.Date(interval60),
weekend = ifelse(dotw %in% c("Sat", "Sun"), 1, 0) %>% as.factor(),
rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0) %>% as.factor()
)
study_panel <- study_panel %>%
left_join(., phl_census_select %>% st_drop_geometry(), by = "GEOID")
study_panel <- study_panel %>%
left_join(., businesses_dens %>% select(-business_cnt) %>% st_drop_geometry(),
by = "GEOID")
study_panel <- study_panel %>%
left_join(weather_complete, by = "interval60")
study_panel <- study_panel %>%
left_join(., holidays_all, by = "date") %>%
rename(holiday = type) %>%
mutate(holiday = replace_na(holiday, "none")) %>%
mutate(holiday = factor(holiday,
levels = c("none",
"holiday",
"festival",
"concert",
"sporting")))
# Sort by station and time
study_panel <- study_panel %>%
arrange(start_station, interval60)
# Create lag variables WITHIN each station
study_panel <- study_panel %>%
group_by(start_station) %>%
mutate(
lag1Hour = lag(Trip_Count, 1),
lag2Hours = lag(Trip_Count, 2),
lag3Hours = lag(Trip_Count, 3),
lag12Hours = lag(Trip_Count, 12),
lag1day = lag(Trip_Count, 24)
) %>%
ungroup()
# Remove rows with NA lags (first 24 hours for each station)
study_panel_complete <- study_panel %>%
filter(!is.na(lag1day))
cat("Rows after removing NA lags: ",
format(nrow(study_panel_complete), big.mark = ","), "\n\n",
"Percent data loss relative to original panel dataset after removing NA lags: ",
round((1-nrow(study_panel_complete)/nrow(study_panel))*100, 2), "%", sep = "", "\n")
# isolate data from one week in a high ridership week to view lag trends
example_station <- study_panel_complete %>%
filter(start_station == 3328 & week == 25)
# Plot actual vs lagged demand
ggplot(example_station, aes(x = interval60)) +
geom_line(aes(y = Trip_Count, color = "Current"), linewidth = 1) +
geom_line(aes(y = lag1Hour, color = "1 Hour Ago"), linewidth = 1, alpha = 0.7) +
geom_line(aes(y = lag1day, color = "24 Hours Ago"), linewidth = 1, alpha = 0.7) +
scale_color_manual(values = c(
"Current" = "#08519c",
"1 Hour Ago" = "#3182bd",
"24 Hours Ago" = "#6baed6"
)) +
labs(
title = "Temporal Lag Patterns at One Station",
subtitle = "Past demand predicts future demand",
x = "Date-Time",
y = "Trip Count",
color = "Time Period"
) +
theme_minimal()
spc_2024 <- study_panel_complete %>% filter(year(interval60) == 2024)
spc_2025 <- study_panel_complete %>% filter(year(interval60) == 2025)
# Which stations have trips in BOTH early and late periods?
early_stations <- spc_2024 %>%
filter(week < 40) %>%
filter(Trip_Count > 0) %>%
distinct(start_station) %>%
pull(start_station)
late_stations <- spc_2024 %>%
filter(week >= 40) %>%
filter(Trip_Count > 0) %>%
distinct(start_station) %>%
pull(start_station)
new_stations <- spc_2025 %>%
filter(Trip_Count > 0) %>%
distinct(start_station) %>%
pull(start_station)
# Keep only stations that appear in BOTH periods
common_stations <- intersect(early_stations, late_stations)
# eliminate trip counts from stations that only have trips in either the train/test data
study_panel_complete_filt <- study_panel_complete %>%
filter(start_station %in% common_stations)
cat("Percent data loss relative to original panel dataset after removing time-limited station counts: ",
round((1-nrow(study_panel_complete_filt)/nrow(study_panel_complete))*100, 2),
"%",
sep = "",
"\n")
spc_2024_filt <- study_panel_complete_filt %>% filter(year(interval60) == 2024)
spc_2025_filt <- study_panel_complete_filt %>% filter(year(interval60) == 2025)
# NOW create train/test split
train <- spc_2024_filt %>%
filter(week < 40)
test <- spc_2024_filt %>%
filter(week >= 40)
cat("Training observations:", format(nrow(train), big.mark = ","), "\n",
"Testing observations:", format(nrow(test), big.mark = ","), "\n",
"Training date range:", as.character(min(train$date)), "to", as.character(max(train$date)), "\n",
"Testing date range:", as.character(min(test$date)), "to", as.character(max(test$date)), "\n")
# Set contrasts to treatment coding (dummy variables)
contrasts(train$dotw_simple) <- contr.treatment(7)
# Now run the model
model1 <- lm(
Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec,
data = train
)
summary(model1)
model2 <- lm(
Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +
lag1Hour + lag3Hours + lag1day,
data = train
)
summary(model2)
model3 <- lm(
Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +
lag1Hour + lag3Hours + lag1day +
Med_Inc + Percent_Taking_Transit + Percent_White,
data = train
)
summary(model3)
model4 <- lm(
Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +
lag1Hour + lag3Hours + lag1day +
Med_Inc + Percent_Taking_Transit + Percent_White +
as.factor(start_station),
data = train
)
# Summary too long with all station dummies, just show key metrics
cat("Model 4 R-squared:", summary(model4)$r.squared, "\n")
cat("Model 4 Adj R-squared:", summary(model4)$adj.r.squared, "\n")
model5 <- lm(
Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +
lag1Hour + lag3Hours + lag1day + rush_hour +
Med_Inc + Percent_Taking_Transit + Percent_White +
as.factor(start_station) +
rush_hour * weekend,  # Rush hour effects different on weekends
data = train
)
cat("Model 5 R-squared:", summary(model5)$r.squared, "\n")
cat("Model 5 Adj R-squared:", summary(model5)$adj.r.squared, "\n")
model6 <- lm(
Trip_Count ~
as.factor(hour) + dotw_simple + lag1Hour + lag3Hours + lag1day +
temp + wspd + rain +
Med_Inc + Percent_Taking_Transit + Percent_White +
business_dens + holiday,
data = train
)
summary(model6)
model7 <- glm(
Trip_Count ~
as.factor(hour) + dotw_simple + lag1Hour + lag3Hours + lag1day +
temp + wspd + rain +
Med_Inc + Percent_Taking_Transit + Percent_White +
business_dens + holiday,
family = "poisson",
data = train)
summary(model7)
# Get predictions on test set
# Set contrasts to treatment coding (dummy variables)
contrasts(test$dotw_simple) <- contr.treatment(7)
test <- test %>%
mutate(
pred1 = predict(model1, newdata = test),
pred2 = predict(model2, newdata = test),
pred3 = predict(model3, newdata = test),
pred4 = predict(model4, newdata = test),
pred5 = predict(model5, newdata = test),
pred6 = predict(model6, newdata = test),
pred7 = predict(model7, newdata = test)
)
spc_2025_filt <- spc_2025_filt %>%
mutate(
pred2 = predict(model2, newdata = spc_2025_filt),
pred6 = predict(model6, newdata = spc_2025_filt)
)
# Calculate MAE for each model
mae_results <- data.frame(
Model = c(
"1. Time + Weather",
"2. + Temporal Lags",
"3. + Demographics",
"4. + Station FE",
"5. + Rush Hour Interaction",
"6. Model 3 + Additional Variables",
"7. Poisson Regression of Model 6 Formula",
"8. Predicting for Q1 2025 - Model 2",
"9. Predicting for Q1 2025 - Model 6"
),
MAE = c(
mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),
mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),
mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),
mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),
mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE),
mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE),
mean(abs(test$Trip_Count - test$pred7), na.rm = TRUE),
mean(abs(spc_2025_filt$Trip_Count - spc_2025_filt$pred2), na.rm = TRUE),
mean(abs(spc_2025_filt$Trip_Count - spc_2025_filt$pred6), na.rm = TRUE)
)
)
kable(mae_results,
digits = 2,
caption = "Mean Absolute Error by Model (Test Set)",
col.names = c("Model", "MAE (trips)")) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# check MAE for Q1 2025
test <- test %>%
mutate(
error = Trip_Count - pred2,
abs_error = abs(error),
time_of_day = case_when(
hour < 7 ~ "Overnight",
hour >= 7 & hour < 10 ~ "AM Rush",
hour >= 10 & hour < 15 ~ "Mid-Day",
hour >= 15 & hour <= 18 ~ "PM Rush",
hour > 18 ~ "Evening"
)
)
# Scatter plot by time and day type
ggplot(test, aes(x = Trip_Count, y = pred2)) +
geom_point(alpha = 0.2, color = "#3182bd") +
geom_abline(slope = 1, intercept = 0, color = "red", linewidth = 1) +
geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
facet_grid(weekend ~ time_of_day) +
labs(
title = "Observed vs. Predicted Bike Trips",
subtitle = "Model 2 performance by time period",
x = "Observed Trips",
y = "Predicted Trips",
caption = "Red line = perfect predictions; Green line = actual model fit"
) +
theme_void()
# Calculate station errors
station_errors <- test %>%
filter(!is.na(pred2)) %>%
group_by(start_station, start_lat, start_lon) %>%
summarize(
MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),
avg_demand = mean(Trip_Count, na.rm = TRUE),
.groups = "drop"
) %>%
filter(!is.na(start_lat), !is.na(start_lon))
# Map 1: Prediction Errors
p1 <- ggplot() +
geom_sf(data = phl_census %>% st_transform(4326), fill = "grey95", color = "white", size = 0.1) +
geom_point(
data = station_errors,
aes(x = start_lon, y = start_lat, color = MAE),
size = 3.5,
alpha = 0.2
) +
scale_color_viridis(
option = "plasma",
name = "MAE (trips)",
direction = -1,
breaks = c(0.5, 1.0, 1.5),
labels = c("0.5", "1.0", "1.5")
) +
labs(title = "Prediction Errors") +
mapTheme +
theme(
legend.position = "bottom",
legend.title = element_text(size = 10, face = "bold"),
legend.text = element_text(size = 9),
plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
) +
guides(color = guide_colorbar(
barwidth = 12,
barheight = 1,
title.position = "top",
title.hjust = 0.5
))
# Map 2: Average Demand
p2 <- ggplot() +
geom_sf(data = phl_census %>% st_transform(4326), fill = "grey95", color = "white", size = 0.1) +
geom_point(
data = station_errors,
aes(x = start_lon, y = start_lat, color = avg_demand),
size = 3.5,
alpha = 0.2
) +
scale_color_viridis(
option = "viridis",
name = "Avg Demand (trips/hour)",
direction = -1,
breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),
labels = c("0.5", "1.0", "1.5", "2.0", "2.5")
) +
labs(title = "Average Demand") +
mapTheme +
theme(
legend.position = "bottom",
legend.title = element_text(size = 10, face = "bold"),
legend.text = element_text(size = 9),
plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
) +
guides(color = guide_colorbar(
barwidth = 12,
barheight = 1,
title.position = "top",
title.hjust = 0.5
))
# Combine
grid.arrange(
p1, p2,
ncol = 2
)
# MAE by time of day and day type
temporal_errors <- test %>%
group_by(time_of_day, weekend) %>%
summarize(
MAE = mean(abs_error, na.rm = TRUE),
.groups = "drop"
) %>%
mutate(day_type = ifelse(weekend == 1, "Weekend", "Weekday"))
ggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
labs(
title = "Prediction Errors by Time Period",
subtitle = "When is the model struggling most?",
x = "Time of Day",
y = "Mean Absolute Error (trips)",
fill = "Day Type"
) +
plotTheme +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Join demographic data to station errors
station_errors_demo <- station_errors %>%
left_join(stn_points_filt_panel %>%
select(start_station, GEOID),
by = "start_station") %>%
left_join(phl_census_select %>%
select(GEOID, Med_Inc, Percent_Taking_Transit, Percent_White),
by = "GEOID") %>%
filter(!is.na(Med_Inc))
# Create plots
p1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +
geom_point(alpha = 0.5, color = "#3182bd") +
geom_smooth(method = "lm", se = FALSE, color = "red") +
scale_x_continuous(labels = scales::dollar) +
labs(title = "Errors vs. Median Income", x = "Median Income", y = "MAE") +
plotTheme
p2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +
geom_point(alpha = 0.5, color = "#3182bd") +
geom_smooth(method = "lm", se = FALSE, color = "red") +
labs(title = "Errors vs. Transit Usage", x = "% Taking Transit", y = "MAE") +
plotTheme
p3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +
geom_point(alpha = 0.5, color = "#3182bd") +
geom_smooth(method = "lm", se = FALSE, color = "red") +
labs(title = "Errors vs. Race", x = "% White", y = "MAE") +
plotTheme
grid.arrange(p1, p2, p3, ncol = 2)
setwd('./GitHub/MUSA-5080-Final/')
go to working directory
if(!require(pacman)){install.packages("pacman"); library(pacman, quietly = T)}
p_load(lubridate, sf, tidycensus, tidygeocoder, tidyverse, tigris, tmap, viridis)
set.seed(5746)
# load evictions dataset to investigate
evictions_df <- read_csv("./data/philadelphia_monthly_2020_2021.csv", show_col_types = F)
evictions_df <- evictions_df %>%
filter(GEOID != "sealed")
# generate vectors of all possible months and GEOIDs
months <- seq(as.Date("2020-01-01"),
as.Date("2025-11-01"),
install.packages(c("xfun", "knitr", rmarkdown))
install.packages("xfun")
install.packages("knitr")
install.packages("rmarkdown")
by = "month") %>%
format("%m/%Y")
geoids  <- unique(evictions_df$GEOID)
# expand a grid of GEOID/month combinations
geoids_months <- expand.grid(GEOID = geoids,
month = months,
stringsAsFactors = F)
# identify missing combinations
missing <- geoids_months %>%
anti_join(evictions_df %>% select(GEOID, month),
by = c("GEOID", "month"))
# print result
if (nrow(missing) > 0){
cat("Number of Missing GEOID/Month Combinations:", nrow(missing))
} else{
cat("No GEOID/Month Combinations Missing")
}
# designate FIPS codes for PA and Philly
fips_pa <- 42
fips_phl <- 101
# load tigris census tract boundaries
tracts <- tracts(state = 42, county = 101, year = 2020, progress_bar = F) %>%
select(GEOID) %>%
st_transform(2272)
# join to evictions dataset and make spatial
evictions <- left_join(evictions_df, tracts, by = "GEOID", ) %>%
st_as_sf()
# manufacture month and year columns, convert existing month column to 'date'
# evictions <- evictions %>%
#   rename(date = month) %>%
#   mutate(
#     month = month(my(date)),
#     year = year(my(date)),
#     filings_2020_log = log(filings_2020 + 1)
#   )
# Create proper date format
evictions <- evictions %>%
mutate(
month_date = as.Date(paste0("01/", month), format = "%d/%m/%Y"),
delta = filings_2020 - filings_avg_prepandemic_baseline,
ratio = filings_2020 / filings_avg_prepandemic_baseline
)
View(evictions)
